{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aanil-vitta/machine-learning-/blob/main/IA3_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI534 IA3: Text Classification with BoW, Linear SVM and Naive Bayes\n",
        "\n"
      ],
      "metadata": {
        "id": "zOK-6wmkFuv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Overview:\n",
        "You will use use the sklearn package to implement and compare classic text classification models, practicing principled evluation, tuning and interpretation. Specifically, your work will include:\n",
        "* clean preprocessing pipeline to produce BoW and TF-IDF features\n",
        "* Linear/RBF SVM and Multinomial Naive Bayes classifiers\n",
        "* Hyperparameter sweeps+plots\n",
        "* Error analysis, feature interpretatoin and short reflection."
      ],
      "metadata": {
        "id": "nmsQojI61OQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "The data for this assignment consists of a natural language sentiment dataset sourced from Twitter. The first column indicates sentiment of the tweets (zero representing negative sentiment and one for positive sentiment) and the second column contains the text of the tweets.\n",
        "\n",
        "Two datasets are provided on Canvas: a training set named 'IA3-training.csv' and a validation set named 'IA3-dev.csv'. You will use the training set to build your models and the validation set to tune the parameters and observe their impacts."
      ],
      "metadata": {
        "id": "XTy88JS_qJHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## What you need to submit\n",
        "1. Your completed notebook in ipynb\n",
        "2. a PDF report that includes all code outputs and figures. You can use the code block at the end of the notebook to generate a PDF export of the notebook with the outputs for your report. However, if any figures or outputs are missing, you must either:\n",
        "* Manually add the missing figures to the PDF using a PDF editor or\n",
        "* Copy your notebook contents into a Word or Google Doc, insert the missing outputs there, and export that document as a PDF."
      ],
      "metadata": {
        "id": "uzJPp76s04Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's import the packages needed for this assignment."
      ],
      "metadata": {
        "id": "J7zBMGIZp_Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbconvert > /dev/null 2>&1\n",
        "!pip install pdfkit > /dev/null 2>&1\n",
        "!apt-get install -y wkhtmltopdf > /dev/null 2>&1\n",
        "import os\n",
        "import pdfkit\n",
        "import contextlib\n",
        "import sys\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "uxYTpds7jgwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the data."
      ],
      "metadata": {
        "id": "53t1CXu5VK4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "train_path = '/content/gdrive/My Drive/AI534/IA3-train.csv' # DO NOT MODIFY THIS. Please make sure your data has this exact path\n",
        "val_path = '/content/gdrive/My Drive/AI534/IA3-dev.csv' # DO NOT MODIFY THIS. Please make sure your data has this exact path\n",
        "train_data = pd.read_csv(train_path)\n",
        "val_data = pd.read_csv(val_path)"
      ],
      "metadata": {
        "id": "E6K8DsDcwjS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92813724-3147-4a39-cfef-19bbe1a45ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 0: (10 pts) Preprocessing and Initial Anlaysis\n",
        "In this part, you will take the text of the tweets and convert it to the bag-of-words (BoW) representation that can be processed by the model you will train.\n",
        "\n"
      ],
      "metadata": {
        "id": "VDHJMP7mtF3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üìò Detailed instructions\n",
        "First, you will build your BoW vocabulary using the training set and represent your training set using *tf-idf*.\n",
        "\n",
        "Here ***tf*** stands for term frequecy and is defined as follows for a document $d$ and term $t$:\n",
        "\n",
        "$TF(t,d) = \\frac{\\mbox{Num. of times } t \\mbox{ appears in } d}{\\mbox{Total number of terms in }d}$\n",
        "\n",
        "and ***idf*** stands for inverse document frequency and is defined as follows for a term $t$ and a document collection $D$\n",
        "\n",
        "$IDF(t, D) = \\log(\\frac{N}{|\\{d\\in D: t\\in d\\}|}) $\n",
        "\n",
        "\n",
        "where $N$ is the total number of documents in $D$, and $|\\{d\\in D: t\\in d\\}|$ is the number of documents that contain term $t$.\n",
        "Specifically, you will be using the **TfidfVectorizer** class from the sklearn.feature_extraction.text package (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#). For this assignment, you should set '***stop_words=english***', other parameters can remain at their default values.\n",
        "\n",
        "The specific methods you will need to use include:\n",
        "- **`fit_transform()`** ‚Äî learns the vocabulary and inverse document frequencies from the input documents (fit), and produces their TF‚ÄìIDF representation (transform).\n",
        "- **`transform()`** ‚Äî applies the learned vocabulary and IDF values to new documents to produce their TF‚ÄìIDF representation.\n",
        "\n",
        "Both methods return a sparse matrix representation of the documents, which can be directly used by the SVM package and the Naive Bayes Package for learning. Using a sparse matrix representation is computationally efficient, especially for SVM models."
      ],
      "metadata": {
        "id": "cfxECUTqiziU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöß Task: Build TF-IDF Features & Build Intial Word Insights\n",
        "**Your tasks:**\n",
        "\n",
        "1. Use class `TfidfVectorizer(stop_words='english')` and apply:\n",
        "   - `fit_transform()` to the **training** tweets to learn the vocabulary and IDF weights and produce the TF-IDF representation for training data\n",
        "   - `transform()` to the **validation** tweets using the learned vocabulary and IDF values and produce the TF-IDF representation of the validation data\n",
        "\n",
        "2. Print the shapes of the resulting TF‚ÄìIDF matrices for the training and validation sets.\n",
        "\n",
        "3. On the **training data only**, for each sentiment class (**positive** and **negative**):\n",
        "   - Compute a **cumulative TF‚ÄìIDF score** for each word:\n",
        "   `CumulativeTFIDF(t, C)` = $\\sum_{d \\in C} \\text{TFIDF}(t, d)$\n",
        "\n",
        "     \n",
        "   - Rank all words by this score and list the **top 20 words** in each class\n",
        "\n",
        "4. Compare the two lists and report:\n",
        "   - Words that appear in **both** lists\n",
        "   - Words **distinctive to positive** sentiment\n",
        "   - Words **distinctive to negative** sentiment\n",
        "\n",
        "> ‚úÖ Your output should include matrix shapes, the two ranked word lists (computed from training data only), and the comparison.\n",
        "\n"
      ],
      "metadata": {
        "id": "-6bU2fzp6NXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "KelbPLaAjT5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Questions\n",
        "\n",
        "1. Consider the three sets of words you identified:  \n",
        "   **(a)** common to both classes, **(b)** positive-only, and **(c)** negative-only.  \n",
        "   What patterns do you observe in each group? (e.g., emotional tone, topic, intensity)\n",
        "\n",
        "2. Suppose you train a linear sentiment classifier (e.g., linear SVM).  \n",
        "   How do you expect it to use these words?  \n",
        "   Discuss the likely **sign** and **magnitude** of the weights assigned to\n",
        "   - positive-only words  \n",
        "   - negative-only words  \n",
        "   - words appearing in both classes"
      ],
      "metadata": {
        "id": "sy9Jv2k_p3Gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here.**"
      ],
      "metadata": {
        "id": "lRedNvbKuWGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1. (30 pts) Linear SVM with C sweep\n",
        "\n",
        "You will train **linear SVM** models on the TF‚ÄìIDF features and tune the regularization parameter **C** over\n",
        "$C \\in \\{10^{-2}, 10^{-1}, 10^{0}, 10^{1}, 10^{2}, 10^{3}\\}.$\n",
        "\n",
        "Use `sklearn.svm.SVC` with `kernel='linear'` so that you can access the **number of support vectors** (this is not available in `LinearSVC`). The linear SVM can consume sparse TF‚ÄìIDF matrices directly."
      ],
      "metadata": {
        "id": "hMNtcBhauj2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üöß Task: Linear SVM and parameter sweep\n",
        "Complete the following:\n",
        "\n",
        "1. Train a `SVC(kernel='linear')` model for each  \n",
        "   \n",
        "   $C \\in \\{10^{-2},\\,10^{-1},\\,1,\\,10,\\,10^2,\\,10^3\\}$\n",
        "\n",
        "2. For each $C$:\n",
        "   - Train on the **training** set\n",
        "   - Compute **AUROC** on **training** and **validation** sets  \n",
        "   - Record the **total number of support vectors**\n",
        "\n",
        "3. Create two plots (log-scale on the \\(C\\)-axis):\n",
        "   - **Training vs validation AUROC** as a function of \\(C\\)\n",
        "   - **Total number of support vectors** as a function of \\(C\\)\n",
        "\n",
        "4. Select and report the **best \\(C\\)** based on **validation AUROC**  "
      ],
      "metadata": {
        "id": "I5htnSLMqc3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here."
      ],
      "metadata": {
        "id": "NgrKbR2uEoPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Questions\n",
        "1. As the regularization parameter $C$ increases, what trends do you expect in   (a) training performance and\n",
        "  (b) validation performance?\n",
        "  Explain why in terms of underfitting vs. overfitting.  \n",
        "  Then compare your theoretical expectation to your observed results.  \n",
        "  If they differ, provide a plausible explanation.\n",
        "2. If different $C$ values produce very similar AUROC values, what principle would you use to choose between them, and why?\n",
        "\n",
        "3. How do you expect the number of support vectors to change as $C$ increases? Explain why based on the SVM objective and margin behavior.  \n",
        "Then compare your expectation to your observed results and discuss any discrepancies.\n",
        "\n"
      ],
      "metadata": {
        "id": "lHI18q2nEqIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here**"
      ],
      "metadata": {
        "id": "Dq9HRG17FTsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöß Task ‚Äî Further Exploration of the SVM Parameter \\(C\\)\n",
        "\n",
        "After completing your initial coarse grid search for \\(C\\), refine your search to more precisely identify an effective value using validation AUROC as the evaluation criterion.\n",
        "\n",
        "Use the following guidelines:\n",
        "\n",
        "- **Boundary Expansion**  \n",
        "  If the best-performing $C$ value lies at the **edge** of your grid (e.g., the smallest or largest value tested), **extend the search range outward** to explore more extreme values.  \n",
        "  This checks whether better performance may exist beyond your initial limits.\n",
        "\n",
        "- **Local Refinement**  \n",
        "  If the best-performing $C$ value lies **within the interior** of your grid (not at an edge), **refine the search locally** by testing intermediate values near the current best.  \n",
        "  For example, if $10^1$ performs best in your initial grid, consider trying nearby values such as $10^{0.5}$ and $10^{1.5}$.\n",
        "\n",
        "- **Iterative Adjustment**  \n",
        "  Apply expansion or refinement **iteratively**.  \n",
        "  If a newly tested value becomes the best, repeat the decision process:\n",
        "  - If the new best lies at a boundary ‚Üí expand further  \n",
        "  - If the new best lies inside the range ‚Üí refine locally again  \n",
        "\n",
        "Continue exploring until additional adjustments **do not meaningfully improve** validation AUROC.\n",
        "\n",
        "**Report**\n",
        "- A small table of tried values and **validation AUROC** (sorted best-to-worst).\n",
        "- Final chosen value for $C$\n",
        "- For the best performing linear SVM model, extract the learned feature weights and report the top 10 positive-weight and top 10 negative-weight words"
      ],
      "metadata": {
        "id": "qbuVM4wIFXHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "xVXayLXuG-7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Questions ‚Äî Comparing SVM Feature Weights to TF-IDF Words\n",
        "\n",
        "Compare the top positive and negative SVM-weighted words to the three TF-IDF groups from Part 0: top words for positive class only (positive-only), top words for negative class only (negative-only), and top words for both classes(common):\n",
        "\n",
        "1. Discuss the overlap between SVM-important words and the three lists. Did any **common** TF-IDF words appear among the most important SVM features?  Why might this happen (or not happen)?\n",
        "\n",
        "2. Identify one or two words that were ranked highly in part 0 for positive or negative sentiment, but that **did not** appear among top SVM features.  Provide one plausible explanation.\n",
        "\n",
        "3. Identify one or two **new** SVM-important words that did *not* appear prominently in the part 0 lists. Why might these words be especially helpful for classification?\n"
      ],
      "metadata": {
        "id": "oIozWHZjHAyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here**"
      ],
      "metadata": {
        "id": "TfPj1DbWEi9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: (30 pts) Naive Bayes Classifier\n",
        "\n",
        "In this part, you will use the same TF-IDF representation of the tweets and experiment with MultiNomial Naive Bayes classifier.\n",
        "\n",
        "Multinomial Naive Bayes estimates class-conditional word probabilities from word counts. However, if a word never appears in a class in the training data, its estimated probability becomes zero, forcing the entire document probability to zero whenever that word appears.\n",
        "\n",
        "To avoid this, Naive Bayes uses a smoothing parameter ùõº, which controls how many \"fake counts\" we add to each word when estimating probabilities. Larger ùõº value means more \"fake counts\" and heavier smoothing.\n",
        "You will study how different levels of smoothing affect model performance."
      ],
      "metadata": {
        "id": "qNGgSM6YoyMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üößTask: Naive Bayes and smoothing parameters\n",
        "1. Using your TF-IDF features, train a MultinomialNB model for each\n",
        "for each  \n",
        "   \n",
        "   $\\alpha \\in \\{10,1,0.5,0.1,0.05\\}$\n",
        "\n",
        "2. For each ùõº:\n",
        "* Train on the training set\n",
        "* Compute the AUROC on the training and validation sets\n",
        "3. Create a table that report the training/validation AUROC for different $\\alpha$ values.\n",
        "\n",
        "4. Select the best ùõº based on validation AUROC.\n"
      ],
      "metadata": {
        "id": "O4WJi-bagnsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here."
      ],
      "metadata": {
        "id": "Wpj-fa1puZy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Questions\n",
        "1. Based on the role of the smoothing parameter ùõº, (i.e., adding ‚Äúfake counts‚Äù), describe how you expect model performance to change as\n",
        "ùõº varies from very small to large values.\n",
        "Justify your expectation in terms of model behavior, not just the plot.\n",
        "\n",
        "2. Compare your expectation with the observed validation AUROC across\n",
        "ùõº. If the pattern does not perfectly match your prediction, suggest one plausible reason grounded in properties of text data or Naive Bayes assumptions.\n",
        "\n",
        "3. Suppose two values of ùõº produce very similar validation AUROC.\n",
        "In such a situation, what principle would you use to choose between them, and why?"
      ],
      "metadata": {
        "id": "uKMWOhjyucSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here**"
      ],
      "metadata": {
        "id": "0Mp4R8dcu5kj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üößTask ‚Äî Refining the Naive Bayes Smoothing Parameter\n",
        "\n",
        "1. Expand your search for the Naive Bayes smoothing parameter \\( \\alpha \\) using the same boundary-expansion / local-refinement strategy from Part 1. Select the $\\alpha$ that yields the highest **validation AUROC**.\n",
        "\n",
        "2. Extract the weight coefficients of the linear classifier produced by Naive Bayes for different words. Specifically, for each word $w_i$, compute its weight as:$\n",
        "   \\log P(w_i \\mid y=1) - \\log P(w_i \\mid y=0)\n",
        "   $\n",
        "\n",
        "   In sklearn, these values can be obtained via `model.feature_log_prob_`.\n",
        "\n",
        "3. Compare two Naive Bayes models:\n",
        "   - Your **best $\\alpha$** based on validation AUROC\n",
        "   - A **large-smoothing model** that uses a much larger smoothing paramter (e.g., $\\alpha = 10$)\n",
        "\n",
        "   For each model, report the **top 10 positive-weight words** and **top 10 negative-weight words** with their corresponding weight values."
      ],
      "metadata": {
        "id": "vIqdimuBu90y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code goes here"
      ],
      "metadata": {
        "id": "S8PlR7WqyGok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úçÔ∏è Questions\n",
        "1. Compare the most important words learned by the two Naive Bayes models\n",
        "(large $\\alpha$ vs. best $\\alpha$):\n",
        "\n",
        "- What changes do you observe in the top positive/negative words and their weights when smoothing increases or decreases? Why does smoothing affect which words are emphasized?\n",
        "\n",
        "2. Now compare the most influential words from your best Naive Bayes model with those from the best linear SVM in Part 1:\n",
        "- Do you see any noticeable differences between the influential words from the two models? How might these differences influence generalization and robustness?\n"
      ],
      "metadata": {
        "id": "aFFuMorTyN8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer goes here."
      ],
      "metadata": {
        "id": "-cniVjSXy8qA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 (20 pts) Exploring bigram features\n",
        "So far you have trained models using unigram features only (single words).\n",
        "In this part, you will investigate whether including bigrams (word pairs) improves sentiment classification performance.\n",
        "Including bigrams enables the model to capture short phrases and word combinations like:\n",
        "\n",
        "* not good\n",
        "\n",
        "* very happy\n",
        "\n",
        "* delayed flight\n",
        "\n",
        "that single words alone may not fully express."
      ],
      "metadata": {
        "id": "gUzDi8d7oo6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöß Task ‚Äî Bigram Feature Exploration\n",
        "\n",
        "1. Construct and evaluate two TF-IDF feature representations:\n",
        "\n",
        "- Using both Unigrams + Bigrams by setting `ngram_range = (1, 2)`\n",
        "\n",
        "- Using Bigrams only by setting `ngram_range = (2, 2)`\n",
        "\n",
        "2. For each representation,\n",
        "- train both linear SVM and Multinomial Naive Bayes, using the selected parameters from Parts 1 and 2 (note here you are not asked to retune ùê∂ or ùõº as the goal is to isolate the effect of changing the feature space only.).\n",
        "- Compute and report training and validation AUROC for each representation\n",
        "\n",
        "- Extract and report the top positive and negative features (similar to Parts 1 & 2)\n"
      ],
      "metadata": {
        "id": "FwiQqMXh3slr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Your code goes here."
      ],
      "metadata": {
        "id": "py_Io-q49TmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Questions.\n",
        "1. Did including bigrams (in addition to unigrams) improve performance compared to using unigram alone? Why might bigrams help in sentiment classification?\n",
        "\n",
        "2. How did the bigrams-only model perform relative to unigrams+bigrams or unigram-only? Provide an explanation for your observed differences.\n",
        "\n",
        "3. Inspect the most influential bigram features. Provide one example where the bigram carries more sentiment meaning than either unigram alone."
      ],
      "metadata": {
        "id": "oQRrdCT59U3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 4. (10 pts) In-class competition\n",
        "We will host a in-class competition using the IA3 data. To participate in this competition, use the following link: https://www.kaggle.com/t/a6382751cf574a7a85b9e9adb8384777\n",
        "\n",
        "**Model restriction.** For this competition, you are required to use SVM and Naive Bayes models.\n",
        "\n",
        "**Exploration encouraged**. Here are some ideas you are welcome to explore:\n",
        "- **Model variants**  \n",
        "  Try alternative settings for SVM or Naive Bayes, or other simple linear models.\n",
        "\n",
        "- **Feature engineering**  \n",
        "  Add, remove, or transform text features (e.g., character n-grams, stopword decisions, emoji handling).\n",
        "\n",
        "- **Data balancing strategies**  \n",
        "  Try upsampling minority class examples or downsampling the majority class.\n",
        "\n",
        "- **Additional preprocessing**  \n",
        "  Experiment with handling punctuation, URLs, user handles, emojis, or casing.\n",
        "\n",
        "- **Additional hyperparameter tuning**  \n",
        "  If useful, continue refining $C$ or $\\alpha$, or explore related knobs.\n",
        "\n",
        "**Team work.** You should continue working in the same team for this competition. The training and validation data provided on the kaggle site are the same as the IA3 assignment.\n",
        "\n",
        "**Evaluation.** To participate, you will apply your trained/tuned model to the test data provided on kaggle (which does not contian the label column), and generate a prediction score for each example. You can consult the sample submission file on Kaggle for the right format for the submission. The metric used for this competition is AUROC due to imbalanced class distribution.\n",
        "\n",
        "There are two parts to the score you will see on kaggle. The performance reported on the public leaderboard and a score reported on the private leaderboard. The public leader board scores are visible through out the competition and you can use it as an external validation to help you refine your model design and tune the model. The private leader board scores are evaluated using a separate set of test data as the final performance evaluation and will be released only after the competition is closed.\n",
        "\n",
        "**Points and bonus points.** You will get the full 10 points if you\n",
        "\n",
        "- Submitting predictions to the competition (at least one successful submission)\n",
        "\n",
        "- Achieving non-trivial performance (i.e., outperforming a simple baseline ‚Äî not necessarily high ranking)\n",
        "\n",
        "- Completing a brief write-up describing:\n",
        "\n",
        "  - what you tried\n",
        "\n",
        "  - which change(s) had the largest effect on performance (positive or negative)\n",
        "\n",
        "  - what you learned from the process\n",
        "\n",
        "You will get **3 nonus points** if your team **scored top 3** on the private leader board, or entered **the largest number of unique submissions** (unique sores).\n",
        "\n",
        "No late submission. The competition will be closed at 11:59 pm of the due date."
      ],
      "metadata": {
        "id": "AHh7zJx00d8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Kaggle write-up\n",
        "\n",
        "**Team name**:\n",
        "\n",
        "Your report goes here."
      ],
      "metadata": {
        "id": "AuDNeUpv82Ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#running this code block will convert this notebook and its outputs into a pdf report.\n",
        "!jupyter nbconvert --to html /content/gdrive/MyDrive/Colab\\ Notebooks/IA3-2024.ipynb  # you might need to change this path to appropriate value to location your copy of the IA0 notebook\n",
        "\n",
        "input_html = '/content/gdrive/MyDrive/Colab Notebooks/IA3-2024.html' #you might need to change this path accordingly\n",
        "output_pdf = '/content/gdrive/MyDrive/Colab Notebooks/IA3output.pdf' #you might need to change this path or name accordingly\n",
        "\n",
        "# Convert HTML to PDF\n",
        "pdfkit.from_file(input_html, output_pdf)\n",
        "\n",
        "# Download the generated PDF\n",
        "files.download(output_pdf)"
      ],
      "metadata": {
        "id": "VBJ-Uzzk7yIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
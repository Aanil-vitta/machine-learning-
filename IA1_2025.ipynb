{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aanil-vitta/machine-learning-/blob/main/IA1_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI534 Implementation 1\n",
        "**Deadline**: Thursday, Oct. 16, by 11:59pm\n",
        "\n",
        "**Submission Instruction**: Submit 1) your completed notebook in ipynb format, and 2) a PDF export of the completed notebook with outputs (the codeblock at the end of the notebook should automatically produce the pdf file).\n",
        "\n",
        "**Overview** In this assignment, we will implement and experiment with linear regression models to predict house prices based on various features. We will use the same housing data you explored in the warm-up assignment.\n",
        "\n",
        "We will implement two versions, one using the closed-form solution, and one using gradient descent.\n",
        "\n",
        "You may modify the starter code as you see fit, including changing the signatures of functions and adding/removing helper functions. However, please make sure that your TA can understand what you are doing and why."
      ],
      "metadata": {
        "id": "zOK-6wmkFuv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First lets import the necessary packages and configure the notebook environment."
      ],
      "metadata": {
        "id": "aP84hVzY97rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages for PDF export (used at the end of the notebook)\n",
        "!pip install nbconvert > /dev/null 2>&1\n",
        "!pip install pdfkit > /dev/null 2>&1\n",
        "!apt-get install -y wkhtmltopdf > /dev/null 2>&1\n",
        "\n",
        "# Import system and utility libraries\n",
        "import os\n",
        "import pdfkit\n",
        "import contextlib\n",
        "import sys\n",
        "from google.colab import files\n",
        "\n",
        "# Import data science libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# add more imports if necessary"
      ],
      "metadata": {
        "id": "mfC82i5Bw_Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 0: (5 pts) Data and preprocessing\n",
        "\n",
        "---\n",
        "### Data access\n",
        "Follow these steps to access the datasets:\n",
        "1. On Canvas, download the following files:\n",
        "- `IA1_train.csv` (training data)\n",
        "- `IA1_val.csv` (validation data)\n",
        "2. Upload both files to your Google Drive at:\n",
        "```\n",
        "/My Drive/AI534/\n",
        "```\n",
        "3. Mount Google Drive in Colab using the following code block, which assumes specific file paths for your files."
      ],
      "metadata": {
        "id": "7Z0sSMySHB-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "train_path = '/content/gdrive/My Drive/AI534/IA1_train.csv' # DO NOT MODIFY THIS. Please make sure your data has this exact path\n",
        "val_path = '/content/gdrive/My Drive/AI534/IA1_val.csv' # DO NOT MODIFY THIS. Please make sure your data has this exact path"
      ],
      "metadata": {
        "id": "E6K8DsDcwjS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now load the training and validation data."
      ],
      "metadata": {
        "id": "nsuGx9oFxRu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(train_path)\n",
        "val_df = pd.read_csv(val_path)"
      ],
      "metadata": {
        "id": "9uTz8ldzx0EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöß Preprocessing\n",
        "Implement the preprocessing function:\n",
        "1. **Remove** the *ID* column from both training and validation data\n",
        "2. **Extract date components** Convert the 'date' column into 3 numerical features: 'day', 'month' and 'year'\n",
        "3. **Create a new feature 'age_since_renovated'** to replace the inconsistent 'yr_renovated'.  is set to 0 if the house has not been renovated. This creates an inconsistent meaning to the numerical values. Replace it with a new feature called *age_since_renovated*:\n",
        "\n",
        ">if *yr_renovate* != 0\n",
        ">> *age_since_renovated* = *year* - *yr\\_renovated*  \n",
        "\n",
        ">else\n",
        ">> *age\\_since\\_renovated = year - yr\\_built*\n",
        "\n",
        "4. **Normalize features using z-score normalization** (except the target 'price')\n",
        "For each feature 'x':\n",
        "$$ z=\\frac{x-\\mu}{\\sigma} $$\n",
        "\n",
        "where:\n",
        " $\\mu$ is the mean of 'x' in the training set\n",
        " $\\sigma$ is the standard deviation of 'x' in the training set\n",
        "\n",
        "Apply the same $\\mu$ and $\\sigma$ from the training data to normalize both the training and validation data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iQkAKzeSyMds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(train_df, val_df):\n",
        "    # Your code goes here\n",
        "    return X_train, X_val, y_train, y_val"
      ],
      "metadata": {
        "id": "foN0WXP43pdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do a quick testing of your normalization, please\n",
        "1. Estimate and print the new mean and standard deviation of the normalized features for the training data --- this should be 0 and 1 respectively.  \n",
        "2. Estimate and print the new mean and standard deviation of the normalized features for the validation data --- these values will not be 0 and 1, but somewhat close"
      ],
      "metadata": {
        "id": "WmnQIMbV6nkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing\n",
        "X_train, X_val, y_train, y_val = preprocess(train_df, val_df)\n",
        "\n",
        "# Print training set stats\n",
        "print(\"Training set (normalized features):\")\n",
        "print(\"Mean:\", X_train.mean().round(2).to_list())\n",
        "print(\"Std: \", X_train.std().round(2).to_list())\n",
        "\n",
        "# Print validation set stats\n",
        "print(\"\\nValidation set (normalized features):\")\n",
        "print(\"Mean:\", X_val.mean().round(2).to_list())\n",
        "print(\"Std: \", X_val.std().round(2).to_list())\n"
      ],
      "metadata": {
        "id": "KIGpj-8v5aFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ‚úçÔ∏è Question\n",
        "Why is it import to use the same $\\mu$ and $\\sigma$ to perform normalization on the training and validation data? What would happen if we use $\\mu$ and $\\sigma$ estimated using the validation to perform normalization on the validation data?  \n"
      ],
      "metadata": {
        "id": "6_ifN_sv66Lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here:**"
      ],
      "metadata": {
        "id": "LqL-ygV7ewdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 (10 pts) Generate closed-form solution for reference.\n",
        "\n",
        "Before we implement gradient descent, we‚Äôll begin by solving linear regression using the **closed-form solution** as a reference point.\n",
        "\n",
        "Our data now contains 21 numeric features. Including the bias term $w_0$, the learned weight vector should have 22 dimensions.\n",
        "\n"
      ],
      "metadata": {
        "id": "C4djL2J-By8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöß Implement closed-form solution for linear regression\n",
        "Write a function to compute the weight vector for linear regression using the **closed-form solution** (also known as the normal equation):\n",
        "$$\n",
        "\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
        "$$\n",
        "\n",
        "You may use NumPy's build-in matrix operations. For numerical stability, we recommend using `np.linalg.pinv()` when computing the inverse.\n",
        "\n",
        "Your function should take the feature matrix and target vector as input, and return the learned weight vector \\( \\mathbf{w} \\)."
      ],
      "metadata": {
        "id": "hg1ynVYnRsNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def closed_form_linear_regression(X, y):\n",
        "    \"\"\"\n",
        "    Compute weights for linear regression using the closed-form solution.\n",
        "\n",
        "    Args:\n",
        "        X (ndarray): Feature matrix of shape (n_samples, n_features)\n",
        "        y (ndarray): Target vector of shape (n_samples,)\n",
        "\n",
        "    Returns:\n",
        "        w (ndarray): Weight vector of shape (n_features,)\n",
        "    \"\"\"\n",
        "# Your code goes here"
      ],
      "metadata": {
        "id": "sc2kX8JxXLiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöß Apply and evaluate the model\n",
        "1. Use your `closed_form_linear_regression()` function to learn weights from the **training data**.\n",
        "2. Use the learned weights to make predictions on both **training** and **validation** sets.\n",
        "3. Report the **Mean Squared Error (MSE)** for both sets.\n",
        "4. Print the learned weight vector (should have 22 values: 21 features + bias)."
      ],
      "metadata": {
        "id": "IhNJnMh3S55j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code goes here"
      ],
      "metadata": {
        "id": "vvsCJ_8ATgGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Question\n",
        "The learned feature weights are often used  to understand the importance of the features. The sign of the weights indicates if a feature positively or negatively impact the price, and the magnitude suggests the strength of the impact. Does the sign of all the features match your expection based on your common-sense understanding of what makes a house expensive? Please hightlight any surprises from the results.\n"
      ],
      "metadata": {
        "id": "DIn4qhdOftK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here**"
      ],
      "metadata": {
        "id": "0l6zfscrfBgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 (35 pts) Implement and experiment with batch gradient descent\n",
        "\n",
        "In this part, you will implement batch gradient descent for linear regression and experiment with it on the given data."
      ],
      "metadata": {
        "id": "DhGbxDUIWUtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöß Implement 'batch_gradient_descent' function\n",
        "\n",
        "Your function should take following **inputs:**\n",
        "- `X` : training feature matrix (shape: n_samples √ó d)\n",
        "- `y` : target vector (shape: n_samples)\n",
        "- `gamma` : learning rate \\( \\gamma \\)\n",
        "- `T` : number of iterations (epochs)\n",
        "- `epsilon_loss` *(optional)*: convergence threshold for loss \\( \\epsilon_l \\)\n",
        "- `epsilon_grad` *(optional)*: convergence threshold for gradient norm \\( \\epsilon_g \\)\n",
        "\n",
        "It should output:\n",
        "1. 'w': the learned $d+1$ - dimensional weight vector\n",
        "2. 'losses': list of mean squared errors for each training iteration"
      ],
      "metadata": {
        "id": "P_aqamSiUnc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(X, y, gamma, T, epsilon_loss=None, epsilon_grad=None):\n",
        "    \"\"\"\n",
        "    Perform batch gradient descent for linear regression.\n",
        "\n",
        "    Args:\n",
        "        X (ndarray): Feature matrix (n_samples, n_features)\n",
        "        y (ndarray): Target vector (n_samples,)\n",
        "        gamma (float): Learning rate\n",
        "        T (int): Number of iterations (epochs)\n",
        "        epsilon_loss (float, optional): Convergence threshold for loss\n",
        "        epsilon_grad (float, optional): Convergence threshold for gradient norm\n",
        "\n",
        "    Returns:\n",
        "        w (ndarray): Learned weight vector (d+1, includes bias)\n",
        "        losses (list): MSE loss at each epoch\n",
        "    \"\"\"\n",
        "    # Your code goes here"
      ],
      "metadata": {
        "id": "W3SKfBArbO2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöß Experiment with different learning rate\n",
        "Use your 'batch_gradient_descent' function to\n",
        "1. Train models on the training data with learning rates $\\gamma = 10^{-i}$ for $i = 0, 1, 2, 3, 4$.\n",
        "2. Train for up to 3000 iterations (stop early if the loss converges or diverges).\n",
        "3. For each converging (not necessarily converged yet) learning rate, compute and report the final MSE on the **validation set**.\n",
        "4. Plot the **training loss curves** (MSE vs. iterations) for all converging learning rates.\n",
        "   - Use different colors for each learning rate\n",
        "   - Include a legend"
      ],
      "metadata": {
        "id": "c4kKoNUIbi7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "Zigtq7OhbQDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Question\n",
        "\n",
        "Which learning rate leads to the best training and validation MSE respectively? Do you observe better training MSE tend to correpsond to better validation MSE? How is this different from the trend shown on page 52 (or vicinity) of the lecture slides (titled 'danger of using training loss to select M') regarding overfitting? Is there any issue with using training loss to pick learning rate in this case?"
      ],
      "metadata": {
        "id": "QLewEsdZbShd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here.**"
      ],
      "metadata": {
        "id": "LH2gZ8aLkRq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3. More exploration."
      ],
      "metadata": {
        "id": "-_FelVO1km4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3(a). (25 pts) Normalization of features: what is the impact?**\n",
        "In part 1, you were asked to perform z-score normalization of all the features. In this part, we will ask you to first conceptually think about what is the impact this operation on the solution and then use some experiments to varify your conceptual understanding."
      ],
      "metadata": {
        "id": "PPga1ENbSkm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è **Questions.**\n",
        "\n",
        "The normalization process applies a linear transformation to each feature, where the transformed feature $x'$ is simply a linear function of original feature $x$: $x'=\\frac{x-\\mu}{\\sigma}$.\n",
        "\n",
        "Let's disect the influence of this transformation on our learned linear regression model.\n",
        "1. How do you think this transformation will influnce the training and validation MSE we get for the closed-form solution? Why?\n",
        "2. How do you think this will change the magnitude of the weights of the learned model? Why?\n",
        "3. How do you think this will change the convergence behavior of the batch gradient descent algorithm? Why?"
      ],
      "metadata": {
        "id": "PPYb-oTeFt8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here.**"
      ],
      "metadata": {
        "id": "liLbn0htPA3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üößExperimental verification\n",
        "Now please perform the following experiments to verify your answer to the above questions.\n",
        "1. Apply 'closed_form_linear_regression' to training data that did not go through the feature normalization step, and report the learned weights and the resulting training and testing MSEs.\n",
        "\n",
        "2. Apply 'batch_gradient_descent' to training data that did not go through the feature normalization step using different learning rates. Note that the learning rate used in previous section will no longer work here. You will need to search for an appropriate learning rate to get some converging behavior. Plot your MSE loss curve as a function of the epochs once you identify a convergent learning rate.\n",
        "Hint: the learning rate needs to be much, much,much, much, much, much, much smaller (think about each much as an order of manitude) than what was used in part 2). Also unless you let it run for a long time, it is unlikely to converge to the same level of loss values. So use a reasonable upper bound on the # of iterations so that it won't take forever."
      ],
      "metadata": {
        "id": "uti32GaMPGwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "cfqvAxFpkhAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Questions\n",
        "\n",
        "Please revisit the questions above. Does your experiment confirm your expectation?  Can you provide explanations to the observed differences (or lack of differences) between the normalized data and unnormalized data? Based on these observations and your understanding of them, please comment on the benefits of normalizing the input features in learning for linear regressions.\n"
      ],
      "metadata": {
        "id": "IqECgJNNQhzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here**"
      ],
      "metadata": {
        "id": "lAiBYtRZROtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3(b). (15 pts) Explore the impact of correlated features**\n",
        "\n",
        "In the warm up exercise, you all have seen some features are highly correlated with one another. For example, there are multiple squared footage related features that are strongly correlated (e.g., *sqft_above* and *sqrt_living* has a correlation coefficient of 0.878).  This is referred to as multicollinearity phenomeon, where two or more features are correlated.\n",
        "\n",
        "There are numerous consequences from multicollinearity. It makes it more challenging to estimate the weights of the features accurately. The weights may become unstable, and their interpretation becomes less clear.\n",
        "\n",
        "In this part you will work with the pre-processed training set, and perform the following experiments to examine how correlated features affect the stability of learned weights."
      ],
      "metadata": {
        "id": "4-sVWZX-SZzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## üößExperiment to investigate impact of correlated features\n",
        "Conduct following experiments.\n",
        "1. **Create five training subsets**:  Randomly subsample 75% of the orginial preprocessed training set to form five slightly different training sets.\n",
        "2. **Fit models**:  Use your 'closed_form_linear_regression' function to train a linear regression model on each of the five training sets.\n",
        "3. **Report learned weights in a table**:  \n",
        "   - The table should have **five rows** (one for each model)  \n",
        "   - Each column corresponds to a **feature‚Äôs weight**  \n",
        "   - Include a **header row** with the feature names\n",
        "\n",
        "4. **Report the variance of weights across models**:  \n",
        "   Include an additional row to the above table to report for each feature, the variance of its learned weight coefficients across the five models.  This variance serves as a measure of the **stability** of the weight assigned to each feature. Larger variance suggests lower stability.\n",
        "  \n",
        "  Note: We use 5 random training subset here to get a rough sense of weight stability. For more robust analysis, you could increase this to 10 or more runs.\n",
        "\n"
      ],
      "metadata": {
        "id": "bETVJ5sKYSwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "SSoQZp6egyR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Questions\n",
        "Ideally, we want the learned weight coefficients to be **stable across different runs**, as this indicates a more **reliable and interpretable** model.\n",
        "- Based on the variances you computed:\n",
        "  - Do features with **high correlation to others** tend to show **more instability** in their weights across different training subsets?\n",
        "  - What trends do you observe?\n",
        "- Use a **correlation matrix** of the input features to support your observations. Which features appear most correlated?\n",
        "- What implications does this have for interpreting feature importance in your model?\n"
      ],
      "metadata": {
        "id": "vodiypKgh26u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here.**"
      ],
      "metadata": {
        "id": "3xc4Aik1ikzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle competition (10 pts)\n",
        "In this section, you will try to build your best model on the given training data and apply it to the provided test data and submit the predictions for the class-wide competition on Kaggle.\n",
        "\n",
        "**Model restriction.** You must use linear regression (without regularization) as your predictive model. No advanced models, such as Ridge, Lasso, tree-based models, neural networks, or other complex learners are not allowed.\n",
        "\n",
        "**Implementation note.** For this part, you are allowed to use a standard library implementation (e.g., 'sklearn.linear_model.LinearRegression') to speed up experimentation.\n",
        "\n",
        "**Exploration encouraged.** You are encouraged to explore:\n",
        "- feature engineering such as removing, transforming features, constructing new features based on existing ones, using different encoding for the discrete features;\n",
        "- training data filtering/modification such as identifying and removing potential outliers in the training data;\n",
        "- target manipulation such as normalizing, or log transforming the prediction target\n",
        "\n",
        "**Fair play and have fun!** The spirit of this competition is for you to learn how far linear regression can go when paired with thoughtful data preparation.\n",
        "\n",
        "To participate in this competition, use the following link:\n",
        "https://www.kaggle.com/t/7e07d14f327c4ee1babd526d4ccf0701\n",
        "\n",
        "\n",
        "**Team work.** You should continue working in the same team for this competition. Make sure to note in your submission your kaggle team name.\n",
        "\n",
        "**How to sumbit.**\n",
        "Your submission should include the prediction for every test sample. The file must be a CSV with two columns: `id` and `price`.\n",
        "- `id` is the unique identifier for each instance as provided in the test data PA1_test1.csv  \n",
        "- `price` is your predicted result.\n",
        "Your file should start with a header row (`id, price`) and followed by $N$ rows, one per test sample.\n",
        "\n",
        "\n",
        "**Competition evluation. ** The competition has two leaderboards: the public leader board as well as the private leader board. The results on the public leader board are visible through out the competition so that you can tell how well your model works compared to others and use it to pick the best models to make submission for the private leader board. Each team will be allowed to submit 3 entries to be evaluated on the private leaderboard for the final performance. The results on the private leaderboard will be released after the competition is closed.\n",
        "\n",
        "**Points and bonus points.** You will get the full 10 points if you\n",
        "- participate in the competition (successful submissions)\n",
        "\n",
        "- achieve non-trivial performance (outperform some simple baseline)\n",
        "\n",
        "- complete the report on the competition below.\n",
        "\n",
        "You will get **3 nonus points** if your team scored top 3 on the private leader board, or entered the largest number of unique submissions (unique sores).\n",
        "\n",
        "**No late submission.** The competition will be closed at 11:59 pm of the due date. No late submission will be allowed for this portion of the assignment to ensure fairness.\n"
      ],
      "metadata": {
        "id": "Ej_IvVZEkS3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Report on the Kaggle competition\n",
        "\n",
        "1. **Team name**:\n",
        "2. **Exploration Summary:** Brief describe the approches you tried. 3. **Most Impactful Change: ** Which exploration led to the most performance improvement, and why do you think it helped?\n"
      ],
      "metadata": {
        "id": "9ISiBeSLtphi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#running this code block will convert this notebook and its outputs into a pdf report.\n",
        "# ‚ö†Ô∏èALERT! Exporting colab notebooks into a clean figure-inclusive pdf can be unreliable.\n",
        "# Sometimes output figures may not appear in your exported file.\n",
        "#\n",
        "#If this happens, please assemble your report mannually: copy relevant fgures/results\n",
        "# into a separate documents and save as PDF. Be sure to clearly lablel each figure with\n",
        "# the corresponding part number (e.g., Part 3(b)).).\n",
        "\n",
        "!jupyter nbconvert --to html /content/gdrive/MyDrive/Colab\\ Notebooks/IA1-2024.ipynb  # you might need to change this path to appropriate value to location your copy of the IA0 notebook\n",
        "\n",
        "input_html = '/content/gdrive/MyDrive/Colab Notebooks/IA1-2025.html' #you might need to change this path accordingly\n",
        "output_pdf = '/content/gdrive/MyDrive/Colab Notebooks/IA1output.pdf' #you might need to change this path or name accordingly\n",
        "\n",
        "# Convert HTML to PDF\n",
        "pdfkit.from_file(input_html, output_pdf)\n",
        "\n",
        "# Download the generated PDF\n",
        "files.download(output_pdf)"
      ],
      "metadata": {
        "id": "VBJ-Uzzk7yIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}